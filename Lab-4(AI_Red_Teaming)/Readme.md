# Lab: AI Red Teaming Lab - Introduction

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo-path/AI-Red-Teaming-Lab.ipynb)

---

## 🧠 Overview

In this lab, you'll explore **Azure AI's Red Teaming tools** to evaluate how well your language models handle adversarial inputs and edge-case prompts. This is essential for ensuring the **robustness, safety, and reliability** of AI systems in real-world deployments.

Unlike basic evaluation tasks, red teaming simulates **malicious user behavior** and tests how your model responds under pressure using **multiple attack strategies** and **risky prompt categories**.

---

## 📘 What You Will Learn

- ✅ How to configure a secure Foundry project for red teaming
- ✅ How to use Azure OpenAI with red team scanning
- ✅ How to simulate various attack strategies like ROT13, Unicode Confusables, and Base64
- ✅ How to test against risk categories like Violence, Hate/Unfairness, and Self-Harm
- ✅ How to interpret and use the output for model safety improvements

---

## 🖼️ Output Preview

Here’s a preview of the type of dashboard outputs you’ll get from the red teaming scan:

![Red Team Dashboard 1](images/img-2.png)

![Red Team Dashboard 2](images/img-1.png)


